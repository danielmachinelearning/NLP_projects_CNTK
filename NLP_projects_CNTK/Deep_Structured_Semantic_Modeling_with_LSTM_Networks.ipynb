{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNTK 303: Deep Structured Semantic Modeling with LSTM Networks\n",
    "\n",
    "DSSM stands for Deep Structured Semantic Model, or more general, Deep Semantic Similarity Model. DSSM, developed by the MSR Deep Learning Technology Center(DLTC), is a deep neural network (DNN) modeling technique for representing text strings (sentences, queries, predicates, entity mentions, etc.) in a continuous semantic space and modeling semantic similarity between two text strings (e.g., Sent2Vec). DSSM has wide applications including information retrieval and web search ranking ([Huang et al. 2013](https://www.microsoft.com/en-us/research/publication/learning-deep-structured-semantic-models-for-web-search-using-clickthrough-data/); [Shen et al. 2014a](https://www.microsoft.com/en-us/research/publication/learning-semantic-representations-using-convolutional-neural-networks-for-web-search/),[2014b](https://www.microsoft.com/en-us/research/publication/a-latent-semantic-model-with-convolutional-pooling-structure-for-information-retrieval/)), ad selection/relevance, contextual entity search and interestingness tasks ([Gao et al. 2014a](https://www.microsoft.com/en-us/research/publication/modeling-interestingness-with-deep-neural-networks/), question answering ([Yih et al., 2014](https://www.microsoft.com/en-us/research/publication/semantic-parsing-for-single-relation-question-answering/)), image captioning ([Fang et al., 2014](https://arxiv.org/abs/1411.4952)), and machine translation ([Gao et al., 2014b](https://www.microsoft.com/en-us/research/publication/learning-continuous-phrase-representations-for-translation-modeling/)) etc. \n",
    "\n",
    "DSSM can be used to develop latent semantic models that project entities of different types (e.g., queries and documents) into a common low-dimensional semantic space for a variety of machine learning tasks such as ranking and classification. For example, in web search ranking, the relevance of a document given a query can be readily computed as the distance between them in that space. With the latest GPUs from Nvidia, we can train our models on billions of words. Readers that are interested in deep learning for text processing may refer to the tutorial by [He et al., 2014](https://www.microsoft.com/en-us/research/publication/deep-learning-for-natural-language-processing-theory-and-practice-tutorial/).\n",
    "We released the predictors and trained model files of the DSSM (also a.k.a. Sent2Vec).\n",
    "\n",
    "## Goal\n",
    "\n",
    "To develop mechanism such that given a pair of documents say a query and a set of web page documents, the model would map the inputs to a pair of feature vectors in a continuous, low dimensional space where one could compare the semantic similarity between the text strings using the cosine similarity between their vectors in that space.  \n",
    "\n",
    "![](http://kubicode.me/img/Study-With-Deep-Structured-Semantic-Model/dssm_arch.png)\n",
    "\n",
    "In the figure above one can see how given a query ($Q$) and set of documents ($D_1, D_2, \\ldots, D_n$), one can generate latent representation a.k.a. semantic features, which can then be used to generate pairwise distance metric. The metric evaluated can be used for ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the picture above, one can see that the query and the document are each mapped to a term vector. While a [bag of word](https://en.wikipedia.org/wiki/Bag-of-words_model) based modeling is a first step one takes while building NLP models, they are limited in their ability to capture relative positions amongst words. Convolution based, or recurrence based models perform better due to their inherent ability to leverage the positions of words. In this tutorial, we will use a simple illustrative model using LSTM to encode the term vector following the work done by [Palangi et. al.](https://www.microsoft.com/en-us/research/wp-content/uploads/2017/02/LSTM_DSSM_IEEE_TASLP.pdf). \n",
    "\n",
    "In this tutorial, we show you how to build such a network.  We use a small sample from the Question-Answering corpus. Additionally we will use a recurrent network to develop the semantic model as it allows to inherently incorporate the positional information with the word tokens. \n",
    "\n",
    "**Note**: The data set is very small and the emphasis of this tutorial is in showing how to create an end-to-end modeling workflow for the DSSM network and not so much on the specific numerical performance we are able to get on this small data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cntk==2.3.1 from https://cntk.ai/PythonWheel/CPU-Only/cntk-2.3.1-cp35-cp35m-linux_x86_64.whl\n",
      "  Downloading https://cntk.ai/PythonWheel/CPU-Only/cntk-2.3.1-cp35-cp35m-linux_x86_64.whl (162.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 162.4MB 4.9kB/s ta 0:00:011  8% |██▋                             | 13.1MB 2.9MB/s eta 0:00:52    8% |██▋                             | 13.4MB 11.9MB/s eta 0:00:13    9% |██▉                             | 14.6MB 3.0MB/s eta 0:00:49    9% |███▏                            | 15.9MB 3.3MB/s eta 0:00:45    10% |███▍                            | 17.3MB 3.0MB/s eta 0:00:48    11% |███▌                            | 17.9MB 9.0MB/s eta 0:00:16    12% |████                            | 20.6MB 9.1MB/s eta 0:00:16    15% |█████                           | 25.6MB 3.6MB/s eta 0:00:38    16% |█████▏                          | 26.4MB 7.7MB/s eta 0:00:18    19% |██████                          | 30.9MB 1.2MB/s eta 0:01:48    19% |██████▍                         | 32.3MB 14.7MB/s eta 0:00:09    20% |██████▋                         | 33.4MB 2.7MB/s eta 0:00:48    23% |███████▋                        | 38.5MB 3.0MB/s eta 0:00:42    23% |███████▋                        | 38.8MB 9.6MB/s eta 0:00:13    24% |███████▉                        | 39.8MB 3.3MB/s eta 0:00:38    24% |████████                        | 40.4MB 14.9MB/s eta 0:00:09    25% |████████▏                       | 41.6MB 10.8MB/s eta 0:00:12    29% |█████████▍                      | 47.7MB 2.7MB/s eta 0:00:43    29% |█████████▌                      | 48.0MB 12.7MB/s eta 0:00:10    30% |█████████▊                      | 49.1MB 3.4MB/s eta 0:00:34    30% |█████████▊                      | 49.4MB 10.3MB/s eta 0:00:11    30% |██████████                      | 50.4MB 3.1MB/s eta 0:00:36    31% |██████████                      | 50.7MB 14.7MB/s eta 0:00:08    37% |████████████                    | 60.9MB 3.6MB/s eta 0:00:28    37% |████████████                    | 61.4MB 3.1MB/s eta 0:00:33    38% |████████████▍                   | 62.9MB 8.4MB/s eta 0:00:12    39% |████████████▋                   | 64.2MB 8.3MB/s eta 0:00:12    40% |█████████████                   | 65.5MB 3.2MB/s eta 0:00:30    40% |█████████████                   | 65.8MB 14.6MB/s eta 0:00:07    41% |█████████████▏                  | 66.7MB 5.6MB/s eta 0:00:18    48% |███████████████▍                | 78.1MB 6.2MB/s eta 0:00:14    48% |███████████████▋                | 79.1MB 3.8MB/s eta 0:00:23    49% |███████████████▉                | 80.3MB 4.1MB/s eta 0:00:21    53% |█████████████████▏              | 87.1MB 2.5MB/s eta 0:00:30    54% |█████████████████▍              | 88.2MB 3.5MB/s eta 0:00:22    54% |█████████████████▍              | 88.4MB 2.9MB/s eta 0:00:26    54% |█████████████████▌              | 88.7MB 10.8MB/s eta 0:00:07    55% |█████████████████▊              | 89.7MB 3.1MB/s eta 0:00:24    56% |██████████████████▏             | 92.3MB 5.0MB/s eta 0:00:15    58% |██████████████████▊             | 95.0MB 12.8MB/s eta 0:00:06    62% |████████████████████▏           | 102.2MB 2.7MB/s eta 0:00:23    63% |████████████████████▍           | 103.7MB 10.7MB/s eta 0:00:06    64% |████████████████████▋           | 104.6MB 3.2MB/s eta 0:00:19    64% |████████████████████▋           | 104.7MB 1.9MB/s eta 0:00:31    65% |█████████████████████           | 106.3MB 11.0MB/s eta 0:00:06    67% |█████████████████████▊          | 110.2MB 7.9MB/s eta 0:00:07    68% |██████████████████████          | 111.6MB 8.1MB/s eta 0:00:07    74% |███████████████████████▉        | 120.8MB 5.7MB/s eta 0:00:08    74% |███████████████████████▉        | 121.0MB 3.7MB/s eta 0:00:12    74% |████████████████████████        | 121.6MB 6.6MB/s eta 0:00:07    76% |████████████████████████▍       | 123.8MB 7.6MB/s eta 0:00:06    77% |████████████████████████▋       | 125.1MB 3.5MB/s eta 0:00:11    77% |████████████████████████▊       | 125.3MB 8.0MB/s eta 0:00:05    83% |██████████████████████████▊     | 135.8MB 14.4MB/s eta 0:00:02    84% |███████████████████████████     | 136.8MB 5.0MB/s eta 0:00:06    85% |███████████████████████████▍    | 139.1MB 5.1MB/s eta 0:00:05    88% |████████████████████████████▍   | 144.2MB 6.3MB/s eta 0:00:03    98% |███████████████████████████████▌| 160.0MB 4.9MB/s eta 0:00:01    98% |███████████████████████████████▊| 160.8MB 12.4MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: cntk\n",
      "  Found existing installation: cntk 2.0\n",
      "    Uninstalling cntk-2.0:\n",
      "      Successfully uninstalled cntk-2.0\n",
      "Successfully installed cntk-2.3.1\n"
     ]
    }
   ],
   "source": [
    "# Upgrade to CNTK 2.3.1\n",
    "!pip install --upgrade --no-deps https://cntk.ai/PythonWheel/CPU-Only/cntk-2.3.1-cp35-cp35m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import the relevant libraries\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n",
    "\n",
    "import cntk as C\n",
    "import cntk.tests.test_utils\n",
    "cntk.tests.test_utils.set_device_from_pytest_env() # (only needed for our build system)\n",
    "C.cntk_py.set_fixed_random_seed(1) # fix a random seed for CNTK components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "### Download\n",
    "\n",
    "We use a sampling of the Question Answering data set for illustrating how to model DSSM networks. The data set consists of pair of sentences with [Questions and Answers](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ACL15-STAGG.pdf). In this tutorial, we have preprocessed the data into two parts:\n",
    "- Vocabulary files: 1 file each for question and answers. There are 1204 and 1019 words in the question and answers vocabulary, respectively.\n",
    "- QA files: 1 file each for training and validation data (hold-out) where each of the files are converted in the [CTF format](https://cntk.ai/pythondocs/CNTK_202_Language_Understanding.html). The training and validation files have 3500 and 409 sentence pairs respectively.\n",
    "\n",
    "Note: a small portion of the original data was provided by the author of the paper for creating an exemplar network for illustration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing locally cached: data/DSSM/train.pair.tok.ctf\n",
      "Reusing locally cached: data/DSSM/valid.pair.tok.ctf\n",
      "Reusing locally cached: data/DSSM/vocab_Q.wl\n",
      "Reusing locally cached: data/DSSM/vocab_A.wl\n"
     ]
    }
   ],
   "source": [
    "location = os.path.normpath('data/DSSM')\n",
    "data = {\n",
    "  'train': { 'file': 'train.pair.tok.ctf' },\n",
    "  'val':{ 'file': 'valid.pair.tok.ctf' },\n",
    "  'query': { 'file': 'vocab_Q.wl' },\n",
    "  'answer': { 'file': 'vocab_A.wl' }\n",
    "}\n",
    "\n",
    "import requests\n",
    "\n",
    "def download(url, filename):\n",
    "    \"\"\" utility function to download a file \"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(filename, \"wb\") as handle:\n",
    "        for data in response.iter_content():\n",
    "            handle.write(data)\n",
    "\n",
    "if not os.path.exists(location):\n",
    "    cwd = os.getcwd()\n",
    "    print(cwd)\n",
    "    print(location)\n",
    "    #os.mkdir(location)\n",
    "    os.makedirs(location)\n",
    "     \n",
    "for item in data.values():\n",
    "    path = os.path.normpath(os.path.join(location, item['file']))\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        print(\"Reusing locally cached:\", path)\n",
    "        \n",
    "    else:\n",
    "        print(\"Starting download:\", item['file'])\n",
    "        url = \"http://www.cntk.ai/jup/dat/DSSM/%s.csv\"%(item['file'])\n",
    "        print(url)\n",
    "        download(url, path)\n",
    "        print(\"Download completed\")\n",
    "    item['file'] = path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reader\n",
    "\n",
    "We will be using the CTF deserializer to read the input data. However, one can write their own readers or use numpy arrays to provide data into CNTK modeling workflow. You may want to open the CTF files with a text editor to parse the input. Note, the CTF deserializer has the capability to scale across production scale data sizes spanning mulitple disks. The reader also abstracts the randomization of the large scale with a simple flag, an added convenience and time savings for the programmer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vocabulary size (QRY-stands for question and ANS stands for answer)\n",
    "QRY_SIZE = 1204\n",
    "ANS_SIZE = 1019\n",
    "\n",
    "def create_reader(path, is_training):\n",
    "    return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "         query = C.io.StreamDef(field='S0', shape=QRY_SIZE,  is_sparse=True),\n",
    "         answer  = C.io.StreamDef(field='S1', shape=ANS_SIZE, is_sparse=True)\n",
    "     )), randomize=is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/DSSM/train.pair.tok.ctf\n",
      "data/DSSM/valid.pair.tok.ctf\n"
     ]
    }
   ],
   "source": [
    "train_file = data['train']['file']\n",
    "print(train_file)\n",
    "\n",
    "if os.path.exists(train_file):\n",
    "    train_source = create_reader(train_file, is_training=True)\n",
    "else:\n",
    "    raise ValueError(\"Cannot locate file {0} in current directory {1}\".format(train_file, os.getcwd()))\n",
    "\n",
    "validation_file = data['val']['file']\n",
    "print(validation_file)\n",
    "if os.path.exists(validation_file):\n",
    "    val_source = create_reader(validation_file, is_training=False)\n",
    "else:\n",
    "    raise ValueError(\"Cannot locate file {0} in current directory {1}\".format(validation_file, os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation\n",
    "\n",
    "The proposed LSTM-RNN model sequentially takes each word in a sentence, extracts its information, and embeds it into a semantic vector. Due to its ability to capture long term memory, the LSTM-RNN accumulates increasingly richer information as it goes through the sentence, and when it reaches the last word, the hidden layer of the network provides a semantic representation of the whole sentence. The `last` block is then projected to a `query_vector` space, also referred to semantic feature in the figure above.\n",
    "\n",
    "\n",
    "                                                                        \"query vector\"\n",
    "                                                                              ^\n",
    "                                                                              |\n",
    "                                                                          +-------+  \n",
    "                                                                          | Dense |  \n",
    "                                                                          +-------+  \n",
    "                                                                              ^         \n",
    "                                                                              |         \n",
    "                                                                         +---------+  \n",
    "                                                                         | Dropout |  \n",
    "                                                                         +---------+\n",
    "                                                                              ^\n",
    "                                                                              |         \n",
    "                                                                          +-------+  \n",
    "                                                                          | Dense |  \n",
    "                                                                          +-------+  \n",
    "                                                                              ^         \n",
    "                                                                              |         \n",
    "                                                                          +------+   \n",
    "                                                                          | last |  \n",
    "                                                                          +------+  \n",
    "                                                                              ^  \n",
    "                                                                              |         \n",
    "                              +------+   +------+   +------+   +------+   +------+   \n",
    "                         0 -->| LSTM |-->| LSTM |-->| LSTM |-->| LSTM |-->| LSTM |\n",
    "                              +------+   +------+   +------+   +------+   +------+   \n",
    "                                  ^          ^          ^          ^          ^\n",
    "                                  |          |          |          |          |\n",
    "                              +-------+  +-------+  +-------+  +-------+  +-------+\n",
    "                              | Embed |  | Embed |  | Embed |  | Embed |  | Embed | \n",
    "                              +-------+  +-------+  +-------+  +-------+  +-------+\n",
    "                                  ^          ^          ^          ^          ^\n",
    "                                  |          |          |          |          |\n",
    "                    query  ------>+--------->+--------->+--------->+--------->+\n",
    "    \n",
    " \n",
    " Similarly we can project the answer sentence to `answer_vector`. However, before we create our model. Let us define the input variables for our model. Note, there is a query and paired with it there is an answer. Given both of these are a sequence of words we define "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the containers for input feature (x) and the label (y)\n",
    "qry = C.sequence.input_variable(QRY_SIZE)\n",
    "ans = C.sequence.input_variable(ANS_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice**: Do you smell any problem with the aforementioned statements. If you want to see what would happen if you were to go with the declarations above, please comment out the 4 statements below and run the model. You will find that your model throws an exception. The details of the exception is explained [here](https://cntk.ai/pythondocs/Manual_How_to_debug.html#Runtime-errors).\n",
    "\n",
    "Each sequence in CNTK, is associated with a dynamic axis representing the number of words in the sequence. Intuitively, when you have sequences of different sizes and vocabularies, each of them need to have their own dynamic axis. This is facilitated by declaring the input data containers with a named axis. Strictly speaking you could name just one, the other one would be a default dynamic axis. However, for clarity we name the two axis separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the containers for input feature (x) and the label (y)\n",
    "axis_qry = C.Axis.new_unique_dynamic_axis('axis_qry')\n",
    "qry = C.sequence.input_variable(QRY_SIZE, sequence_axis=axis_qry)\n",
    "\n",
    "axis_ans = C.Axis.new_unique_dynamic_axis('axis_ans')\n",
    "ans = C.sequence.input_variable(ANS_SIZE, sequence_axis=axis_ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can create the model we need to specify a few parameters associated with the network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIM   = 25 # Embedding dimension\n",
    "HIDDEN_DIM = 50 # LSTM dimension\n",
    "DSSM_DIM = 25 # Dense layer dimension  \n",
    "NEGATIVE_SAMPLES = 5\n",
    "DROPOUT_RATIO = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(qry, ans):\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "        qry_vector = C.layers.Sequential([\n",
    "            C.layers.Embedding(EMB_DIM, name='embed'),\n",
    "            C.layers.Recurrence(C.layers.LSTM(HIDDEN_DIM), go_backwards=False),\n",
    "            C.sequence.last,\n",
    "            C.layers.Dense(DSSM_DIM, activation=C.relu, name='q_proj'),\n",
    "            C.layers.Dropout(DROPOUT_RATIO, name='dropout qdo1'),\n",
    "            C.layers.Dense(DSSM_DIM, activation=C.tanh, name='q_enc')\n",
    "        ])\n",
    "        \n",
    "        ans_vector = C.layers.Sequential([\n",
    "            C.layers.Embedding(EMB_DIM, name='embed'),\n",
    "            C.layers.Recurrence(C.layers.LSTM(HIDDEN_DIM), go_backwards=False),\n",
    "            C.sequence.last,\n",
    "            C.layers.Dense(DSSM_DIM, activation=C.relu, name='a_proj'),\n",
    "            C.layers.Dropout(DROPOUT_RATIO, name='dropout ado1'),\n",
    "            C.layers.Dense(DSSM_DIM, activation=C.tanh, name='a_enc')\n",
    "        ])\n",
    "\n",
    "    return {\n",
    "        'query_vector': qry_vector(qry),\n",
    "        'answer_vector': ans_vector(ans)\n",
    "    }\n",
    "\n",
    "# Create the model and store reference in `network` dictionary\n",
    "network = create_model(qry, ans)\n",
    "\n",
    "network['query'], network['axis_qry'] = qry, axis_qry\n",
    "network['answer'], network['axis_ans'] = ans, axis_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now that we have created a network, the next step is to find a suitable loss function where if a `question` is paired with the correct `answer`, the loss would be 0 else it would be 1. In other words, this loss should maximize the similarity (dot  product) between the answer vector which appears close to the answer vector  and minimize the similarity of between the answer and question vector that do not answer each other.  \n",
    "\n",
    "The use cases of DSSM often appear in information retrieval where for a given query or question there are few answers amongst an ocean of poor or non-answers. The input data as in this case is a pair of query and answer (document or advertisement) that attracted a click. A classical way to train would be a a binary classifier to predict click / no-click (or equivalently a 2-class classifier - one class each for click or no click). One could generate pairs of query and incorrect answers (as no-click data). However, one way to simulate no-click data is to use query and answers for other queries within a minibatch. This is the concept behind `cosine_distance_with_negative_samples` function. Note: This function returns 1 for correct the question and answer pair and 0 for incorrect, which is referred to as *similarity*. Hence, we use 1- `cosine_distance_with_negative_samples` as our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loss(vector_a, vector_b):\n",
    "    qry_ans_similarity = C.cosine_distance_with_negative_samples(vector_a, \\\n",
    "                                                                 vector_b, \\\n",
    "                                                                 shift=1, \\\n",
    "                                                                 num_negative_samples=5)\n",
    "    return 1 - qry_ans_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "MAX_EPOCHS = 40\n",
    "EPOCH_SIZE = 50000\n",
    "MINIBATCH_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "def create_trainer(reader, network):\n",
    "    \n",
    "    # Setup the progress updater\n",
    "    progress_writer = C.logging.ProgressPrinter(tag='Training', num_epochs=MAX_EPOCHS)\n",
    "\n",
    "    # Set learning parameters\n",
    "    lr_per_sample     = [0.0015625]*20 + \\\n",
    "                        [0.00046875]*20 + \\\n",
    "                        [0.00015625]*20 + \\\n",
    "                        [0.000046875]*10 + \\\n",
    "                        [0.000015625]\n",
    "    lr_schedule       = C.learning_parameter_schedule_per_sample(lr_per_sample, \\\n",
    "                                                 epoch_size=EPOCH_SIZE)\n",
    "    mms               = [0]*20 + [0.9200444146293233]*20 + [0.9591894571091382]\n",
    "    mm_schedule       = C.learners.momentum_schedule(mms, \\\n",
    "                                                     epoch_size=EPOCH_SIZE, \\\n",
    "                                                     minibatch_size=MINIBATCH_SIZE)\n",
    "    l2_reg_weight     = 0.0002\n",
    "\n",
    "    model = C.combine(network['query_vector'], network['answer_vector'])\n",
    "\n",
    "    #Notify the network that the two dynamic axes are indeed same\n",
    "    query_reconciled = C.reconcile_dynamic_axes(network['query_vector'], network['answer_vector'])\n",
    "  \n",
    "    network['loss'] = create_loss(query_reconciled, network['answer_vector'])\n",
    "    network['error'] = None\n",
    "\n",
    "    print('Using momentum sgd with no l2')\n",
    "    dssm_learner = C.learners.momentum_sgd(model.parameters, lr_schedule, mm_schedule)\n",
    "\n",
    "    network['learner'] = dssm_learner\n",
    " \n",
    "    print('Using local learner')\n",
    "    # Create trainer\n",
    "    return C.Trainer(model, (network['loss'], network['error']), network['learner'], progress_writer)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using momentum sgd with no l2\n",
      "Using local learner\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the trainer\n",
    "trainer = create_trainer(train_source, network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train \n",
    "def do_train(network, trainer, train_source):\n",
    "    # define mapping from intput streams to network inputs\n",
    "    input_map = {\n",
    "        network['query']: train_source.streams.query,\n",
    "        network['answer']: train_source.streams.answer\n",
    "        } \n",
    "\n",
    "    t = 0\n",
    "    for epoch in range(MAX_EPOCHS):         # loop over epochs\n",
    "        epoch_end = (epoch+1) * EPOCH_SIZE\n",
    "        while t < epoch_end:                # loop over minibatches on the epoch\n",
    "            data = train_source.next_minibatch(MINIBATCH_SIZE, input_map= input_map)  # fetch minibatch\n",
    "            trainer.train_minibatch(data)               # update model with it\n",
    "            t += MINIBATCH_SIZE\n",
    "\n",
    "        trainer.summarize_training_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate per 1 samples: 0.0015625\n",
      "Momentum per 50 samples: 0.0\n",
      "Finished Epoch[1 of 40]: [Training] loss = 0.000180 * 7622, metric = 0.00% * 7622 6.760s (1127.5 samples/s);\n",
      "Finished Epoch[2 of 40]: [Training] loss = 0.000123 * 7613, metric = 0.00% * 7613 5.290s (1439.1 samples/s);\n",
      "Finished Epoch[3 of 40]: [Training] loss = 0.000086 * 7622, metric = 0.00% * 7622 5.081s (1500.1 samples/s);\n",
      "Finished Epoch[4 of 40]: [Training] loss = 0.000066 * 7614, metric = 0.00% * 7614 5.062s (1504.1 samples/s);\n",
      "Finished Epoch[5 of 40]: [Training] loss = 0.000051 * 7616, metric = 0.00% * 7616 5.388s (1413.5 samples/s);\n",
      "Finished Epoch[6 of 40]: [Training] loss = 0.000039 * 7610, metric = 0.00% * 7610 5.240s (1452.3 samples/s);\n",
      "Finished Epoch[7 of 40]: [Training] loss = 0.000032 * 7605, metric = 0.00% * 7605 5.168s (1471.6 samples/s);\n",
      "Finished Epoch[8 of 40]: [Training] loss = 0.000026 * 7610, metric = 0.00% * 7610 5.275s (1442.7 samples/s);\n",
      "Finished Epoch[9 of 40]: [Training] loss = 0.000022 * 7627, metric = 0.00% * 7627 4.966s (1535.8 samples/s);\n",
      "Finished Epoch[10 of 40]: [Training] loss = 0.000019 * 7633, metric = 0.00% * 7633 5.586s (1366.5 samples/s);\n",
      "Finished Epoch[11 of 40]: [Training] loss = 0.000017 * 7614, metric = 0.00% * 7614 5.212s (1460.9 samples/s);\n",
      "Finished Epoch[12 of 40]: [Training] loss = 0.000014 * 7626, metric = 0.00% * 7626 5.309s (1436.4 samples/s);\n",
      "Finished Epoch[13 of 40]: [Training] loss = 0.000012 * 7616, metric = 0.00% * 7616 5.700s (1336.1 samples/s);\n",
      "Finished Epoch[14 of 40]: [Training] loss = 0.000011 * 7629, metric = 0.00% * 7629 5.030s (1516.7 samples/s);\n",
      "Finished Epoch[15 of 40]: [Training] loss = 0.000010 * 7593, metric = 0.00% * 7593 5.416s (1402.0 samples/s);\n",
      "Finished Epoch[16 of 40]: [Training] loss = 0.000009 * 7608, metric = 0.00% * 7608 5.352s (1421.5 samples/s);\n",
      "Finished Epoch[17 of 40]: [Training] loss = 0.000008 * 7633, metric = 0.00% * 7633 5.569s (1370.6 samples/s);\n",
      "Finished Epoch[18 of 40]: [Training] loss = 0.000007 * 7607, metric = 0.00% * 7607 5.282s (1440.2 samples/s);\n",
      "Finished Epoch[19 of 40]: [Training] loss = 0.000006 * 7614, metric = 0.00% * 7614 5.211s (1461.1 samples/s);\n",
      "Finished Epoch[20 of 40]: [Training] loss = 0.000005 * 7612, metric = 0.00% * 7612 5.032s (1512.7 samples/s);\n",
      "Finished Epoch[21 of 40]: [Training] loss = 0.000005 * 7611, metric = 0.00% * 7611 5.402s (1408.9 samples/s);\n",
      "Finished Epoch[22 of 40]: [Training] loss = 0.000005 * 7594, metric = 0.00% * 7594 5.664s (1340.7 samples/s);\n",
      "Finished Epoch[23 of 40]: [Training] loss = 0.000004 * 7613, metric = 0.00% * 7613 5.185s (1468.3 samples/s);\n",
      "Finished Epoch[24 of 40]: [Training] loss = 0.000004 * 7636, metric = 0.00% * 7636 6.659s (1146.7 samples/s);\n",
      "Finished Epoch[25 of 40]: [Training] loss = 0.000004 * 7603, metric = 0.00% * 7603 7.280s (1044.4 samples/s);\n",
      "Finished Epoch[26 of 40]: [Training] loss = 0.000003 * 7624, metric = 0.00% * 7624 5.639s (1352.0 samples/s);\n",
      "Finished Epoch[27 of 40]: [Training] loss = 0.000003 * 7599, metric = 0.00% * 7599 5.742s (1323.4 samples/s);\n",
      "Finished Epoch[28 of 40]: [Training] loss = 0.000003 * 7631, metric = 0.00% * 7631 5.138s (1485.2 samples/s);\n",
      "Finished Epoch[29 of 40]: [Training] loss = 0.000003 * 7606, metric = 0.00% * 7606 5.017s (1516.0 samples/s);\n",
      "Finished Epoch[30 of 40]: [Training] loss = 0.000002 * 7617, metric = 0.00% * 7617 5.151s (1478.7 samples/s);\n",
      "Finished Epoch[31 of 40]: [Training] loss = 0.000002 * 7619, metric = 0.00% * 7619 5.109s (1491.3 samples/s);\n",
      "Finished Epoch[32 of 40]: [Training] loss = 0.000002 * 7622, metric = 0.00% * 7622 5.332s (1429.5 samples/s);\n",
      "Finished Epoch[33 of 40]: [Training] loss = 0.000002 * 7615, metric = 0.00% * 7615 5.577s (1365.4 samples/s);\n",
      "Finished Epoch[34 of 40]: [Training] loss = 0.000002 * 7589, metric = 0.00% * 7589 5.120s (1482.2 samples/s);\n",
      "Finished Epoch[35 of 40]: [Training] loss = 0.000002 * 7616, metric = 0.00% * 7616 5.369s (1418.5 samples/s);\n",
      "Finished Epoch[36 of 40]: [Training] loss = 0.000002 * 7590, metric = 0.00% * 7590 5.763s (1317.0 samples/s);\n",
      "Finished Epoch[37 of 40]: [Training] loss = 0.000002 * 7626, metric = 0.00% * 7626 5.363s (1422.0 samples/s);\n",
      "Finished Epoch[38 of 40]: [Training] loss = 0.000002 * 7607, metric = 0.00% * 7607 5.435s (1399.6 samples/s);\n",
      "Finished Epoch[39 of 40]: [Training] loss = 0.000002 * 7613, metric = 0.00% * 7613 5.779s (1317.4 samples/s);\n",
      "Finished Epoch[40 of 40]: [Training] loss = 0.000001 * 7632, metric = 0.00% * 7632 5.401s (1413.1 samples/s);\n"
     ]
    }
   ],
   "source": [
    "do_train(network, trainer, train_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate\n",
    "\n",
    "Once the model is trained we want to select a model that has similar error with the validation (hold-out set) as the error with the training set. \n",
    "\n",
    "**Suggested Activity**: Vary the number of epochs and check the training and the validation error.\n",
    "\n",
    "The chosen model would then be used for prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate\n",
    "def do_validate(network, val_source):\n",
    "    # process minibatches and perform evaluation\n",
    "    progress_printer = C.logging.ProgressPrinter(tag='Evaluation', num_epochs=0)\n",
    "\n",
    "    val_map = {\n",
    "        network['query']: val_source.streams.query,\n",
    "        network['answer']: val_source.streams.answer\n",
    "        } \n",
    "\n",
    "    evaluator = C.eval.Evaluator(network['loss'], progress_printer)\n",
    "\n",
    "    while True:\n",
    "        minibatch_size = 100\n",
    "        data = val_source.next_minibatch(minibatch_size, input_map=val_map)\n",
    "        if not data:                                 # until we hit the end\n",
    "            break\n",
    "\n",
    "        evaluator.test_minibatch(data)\n",
    "\n",
    "    evaluator.summarize_test_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Evaluation [1]: Minibatch[1-0]: metric = 0.00% * 0;\n"
     ]
    }
   ],
   "source": [
    "do_validate(network, val_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Predict\n",
    "\n",
    "We will now create a vector representation of the query and the answer. Then compute the cosine similarity between the two vectors. When the answer is close to the question one would get a high similarity, while an incorrect / partially relevant question / answer pair would result in a smaller similarity. These scores are often used for ranking web documents in response to a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Indices: [1202, 1154, 267, 321, 357, 648, 1070, 905, 549, 6, 1203]\n",
      "Answer Indices: [1017, 135, 91, 137, 1018]\n",
      "Poor Answer Indices: [1017, 501, 452, 533, 1018]\n"
     ]
    }
   ],
   "source": [
    "# load dictionaries\n",
    "query_wl = [line.rstrip('\\n') for line in open(data['query']['file'])]\n",
    "answers_wl = [line.rstrip('\\n') for line in open(data['answer']['file'])]\n",
    "query_dict = {query_wl[i]:i for i in range(len(query_wl))}\n",
    "answers_dict = {answers_wl[i]:i for i in range(len(answers_wl))}\n",
    "\n",
    "# let's run a sequence through\n",
    "qry = 'BOS what contribution did  e1  made to science in 1665 EOS'\n",
    "ans = 'BOS book author book_editions_published EOS'\n",
    "ans_poor = 'BOS language human_language main_country EOS'\n",
    "\n",
    "qry_idx = [query_dict[w+' '] for w in qry.split()] # convert to query word indices\n",
    "print('Query Indices:', qry_idx)\n",
    "\n",
    "ans_idx = [answers_dict[w+' '] for w in ans.split()] # convert to answer word indices\n",
    "print('Answer Indices:', ans_idx)\n",
    "\n",
    "ans_poor_idx = [answers_dict[w+' '] for w in ans_poor.split()] # convert to fake answer word indices\n",
    "print('Poor Answer Indices:', ans_poor_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the query, answer and the fake answer to one-hot representation. This is a necessary step since the input to our trained network takes one-hot encoded input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the one hot representations\n",
    "qry_onehot = np.zeros([len(qry_idx),len(query_dict)], np.float32)\n",
    "for t in range(len(qry_idx)):\n",
    "    qry_onehot[t,qry_idx[t]] = 1\n",
    "    \n",
    "ans_onehot = np.zeros([len(ans_idx),len(answers_dict)], np.float32)\n",
    "for t in range(len(ans_idx)):\n",
    "    ans_onehot[t,ans_idx[t]] = 1\n",
    "    \n",
    "ans_poor_onehot = np.zeros([len(ans_poor_idx),len(answers_dict)], np.float32)\n",
    "for t in range(len(ans_poor_idx)):\n",
    "    ans_poor_onehot[t, ans_poor_idx[t]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the query and the answer one-hot encoded input, create the embeddings. Note: we use the answer embedding for both the correct answer and the poor answer. We compute the cosine similarity between the query and answer pair. The relative value of the cosine similarity with a higher value indicating a better answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query to Answer similarity: 0.999998792715\n",
      "Query to poor-answer similarity: 0.999998613019\n"
     ]
    }
   ],
   "source": [
    "qry_embedding = network['query_vector'].eval([qry_onehot])\n",
    "ans_embedding = network['answer_vector'].eval([ans_onehot])\n",
    "ans_poor_embedding = network['answer_vector'].eval([ans_poor_onehot])\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "print('Query to Answer similarity:', 1-cosine(qry_embedding, ans_embedding))\n",
    "print('Query to poor-answer similarity:', 1-cosine(qry_embedding, ans_poor_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
